import torch
import os
import time
from transformers import AutoTokenizer, LlamaForCausalLM

# Mac GPU(MPS) ì‚¬ìš© ì„¤ì •
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
torch.backends.cudnn.benchmark = True  # CUDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ
if device == "mps":
    torch.mps.set_per_process_memory_fraction(0.8)  # MPS ë©”ëª¨ë¦¬ ì œí•œ

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
models_info = {
    "kanana-safeguard-8b": {
        "model_name": "kakaocorp/kanana-safeguard-8b",
        "model_path": "./models/models--kakaocorp--kanana-safeguard-8b",
        "model_type": "LlamaForCausalLM",
        "tokenizer_name": "kakaocorp/kanana-safeguard-8b",
        "tokenizer_path": "./models/models--kakaocorp--kanana-safeguard-8b",
        "config_path": "./models/models--kakaocorp--kanana-safeguard-8b/snapshots/2f4a68641d818caf873e21badcdc161928b0fcbf"
    }
}

# ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ” snapshots ë””ë ‰í† ë¦¬ ì‚¬ìš©
kanana_model_config_path = models_info["kanana-safeguard-8b"]["config_path"]
print(f"ëª¨ë¸ ê²½ë¡œ: {kanana_model_config_path}")

print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
start_time = time.time()

# ë¡œì»¬ ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
if os.path.exists(kanana_model_config_path) and os.path.exists(os.path.join(kanana_model_config_path, "config.json")):
    print(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {kanana_model_config_path}")
    # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ - LlamaForCausalLM ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
    try:
        model = LlamaForCausalLM.from_pretrained(
            kanana_model_config_path,
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",  # Flash Attention 2 ì‚¬ìš© (ë” ë¹ ë¦„)
            local_files_only=True,
            use_cache=True  # KV ìºì‹œ í™œì„±í™”
        ).eval()
    except Exception as e:
        print(f"Flash Attention 2 ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
        # Flash Attentionì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° fallback
        model = LlamaForCausalLM.from_pretrained(
            kanana_model_config_path,
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="eager",
            local_files_only=True,
            use_cache=True
        ).eval()
    
    # í† í¬ë‚˜ì´ì €ë„ ë¡œì»¬ì—ì„œ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(kanana_model_config_path)
    
else:
    print("ë¡œì»¬ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ê²©ì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    # ì›ê²©ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    try:
        model = LlamaForCausalLM.from_pretrained(
            models_info["kanana-safeguard-8b"]["model_name"],
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",
            use_cache=True
        ).eval()
    except Exception as e:
        print(f"Flash Attention 2 ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
        model = LlamaForCausalLM.from_pretrained(
            models_info["kanana-safeguard-8b"]["model_name"],
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            attn_implementation="eager",
            use_cache=True
        ).eval()
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(models_info["kanana-safeguard-8b"]["model_name"])
    
    # ë¡œì»¬ì— ì €ì¥ (ì„ íƒì‚¬í•­)
    if not os.path.exists(kanana_model_config_path):
        print(f"ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥ ì¤‘: {kanana_model_config_path}")
        os.makedirs(kanana_model_config_path, exist_ok=True)
        model.save_pretrained(kanana_model_config_path)
        tokenizer.save_pretrained(kanana_model_config_path)

# ëª¨ë¸ì„ MPS ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
model = model.to(device)

# íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”ì‹œ)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ìµœì í™”
tokenizer.padding_side = "left"  # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

load_time = time.time() - start_time
print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")

def classify(user_prompt: str, assistant_prompt: str = "") -> str:
    """
    ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
    
    Args:
        user_prompt: ì‚¬ìš©ì ì§ˆë¬¸/ìš”ì²­
        assistant_prompt: ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë¶„ë¥˜ ê²°ê³¼ í† í° (ì˜ˆ: <UNSAFE-S4>, <SAFE> ë“±)
    """
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": assistant_prompt}
    ]

    # ì±„íŒ… í…œí”Œë¦¿ ì ìš© í›„ í† í°í™” (ì„±ëŠ¥ ìµœì í™”)
    input_ids = tokenizer.apply_chat_template(
        messages, 
        tokenize=True, 
        return_tensors="pt",
        padding=False,  # íŒ¨ë”© ë¹„í™œì„±í™” (ë‹¨ì¼ ì¶”ë¡  ì‹œ ë¶ˆí•„ìš”)
        truncation=True,  # ê¸´ ì…ë ¥ ìë¥´ê¸°
        max_length=512  # ìµœëŒ€ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì†ë„ í–¥ìƒ
    ).to(device)
    
    attention_mask = (input_ids != tokenizer.pad_token_id).long()

    # ë‹¤ìŒ í† í° 1ê°œ ìƒì„± (ì¶”ë¡ ) - ì„±ëŠ¥ ìµœì í™”
    with torch.no_grad():
        with torch.autocast(device_type=device.split(':')[0] if ':' in device else device, dtype=torch.float16):
            output_ids = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=1,
                min_new_tokens=1,  # ìµœì†Œ í† í° ìˆ˜ ë³´ì¥
                do_sample=False,  # ê·¸ë¦¬ë”” ë””ì½”ë”© (ë” ë¹ ë¦„)
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True,  # KV ìºì‹œ ì‚¬ìš©
                temperature=None,  # ì˜¨ë„ ë¹„í™œì„±í™” (ê·¸ë¦¬ë””)
                top_p=None,  # top-p ë¹„í™œì„±í™”
                top_k=None,  # top-k ë¹„í™œì„±í™”
                repetition_penalty=None,  # ë°˜ë³µ í˜ë„í‹° ë¹„í™œì„±í™”
                num_beams=1,  # ë¹” ì„œì¹˜ ë¹„í™œì„±í™”
                early_stopping=True  # ì¡°ê¸° ì¢…ë£Œ
            )

    # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ì¶”ì¶œí•´ ë””ì½”ë”©
    gen_idx = input_ids.shape[-1]
    return tokenizer.decode(output_ids[0][gen_idx], skip_special_tokens=True)

def get_safety_description(token: str) -> str:
    """ì•ˆì „ì„± í† í°ì— ëŒ€í•œ ì„¤ëª…ì„ ë°˜í™˜"""
    descriptions = {
        "<SAFE>": "ì•ˆì „í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S1>": "ë¶ˆë²• í™œë™ ê´€ë ¨ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S2>": "í˜ì˜¤ ë°œì–¸ ê´€ë ¨ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S3>": "ì„±ì  ë‚´ìš© ê´€ë ¨ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S4>": "ìí•´/í­ë ¥ ê´€ë ¨ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S5>": "ê°œì¸ì •ë³´ ì¹¨í•´ ê´€ë ¨ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "<UNSAFE-S6>": "ê¸°íƒ€ ìœ„í—˜ ë‚´ìš©ì…ë‹ˆë‹¤."
    }
    return descriptions.get(token, "ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ë¥˜ ê²°ê³¼ì…ë‹ˆë‹¤.")

# ì›œì—… ì‹¤í–‰ (ì²« ë²ˆì§¸ ì¶”ë¡  ì†ë„ í–¥ìƒ)
print("ğŸ”¥ ëª¨ë¸ ì›œì—… ì¤‘...")
warmup_start = time.time()
_ = classify("í…ŒìŠ¤íŠ¸", "")
warmup_time = time.time() - warmup_start
print(f"âœ… ì›œì—… ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {warmup_time:.2f}ì´ˆ)")

def main():
    """ë©”ì¸ ëŒ€í™” ë£¨í”„"""
    print("\n" + "="*60)
    print("ğŸ›¡ï¸  Kanana Safeguard ëŒ€í™”í˜• ì•ˆì „ì„± ê²€ì‚¬ ì‹œìŠ¤í…œ (ê³ ì† ë²„ì „)")
    print("="*60)
    print("ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì˜ ì•ˆì „ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("="*60)
    
    while True:
        try:
            print("\n" + "-"*40)
            # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
            user_input = input("ğŸ‘¤ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if user_input.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì•ˆì „ì„± ê²€ì‚¬ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                print("âš ï¸  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì…ë ¥ (ì„ íƒì‚¬í•­)
            assistant_input = input("ğŸ¤– ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ (ì„ íƒì‚¬í•­, Enterë¡œ ê±´ë„ˆë›°ê¸°): ").strip()
            
            # ì•ˆì „ì„± ë¶„ë¥˜ ìˆ˜í–‰ (ì‹œê°„ ì¸¡ì •)
            print("\nğŸ” ì•ˆì „ì„± ê²€ì‚¬ ì¤‘...")
            inference_start = time.time()
            result_token = classify(user_input, assistant_input)
            inference_time = time.time() - inference_start
            description = get_safety_description(result_token)
            
            # ê²°ê³¼ ì¶œë ¥ (ì„±ëŠ¥ ì •ë³´ í¬í•¨)
            print(f"\nğŸ“Š ê²€ì‚¬ ê²°ê³¼: (ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ)")
            print(f"   ë¶„ë¥˜ í† í°: {result_token}")
            print(f"   ì„¤ëª…: {description}")
            
            # ì•ˆì „ì„± ë ˆë²¨ì— ë”°ë¥¸ ì¶”ê°€ ì •ë³´
            if result_token == "<SAFE>":
                print("   âœ… ì•ˆì „í•œ ëŒ€í™”ì…ë‹ˆë‹¤.")
            else:
                print("   âš ï¸  ì£¼ì˜ê°€ í•„ìš”í•œ ë‚´ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì•ˆì „ì„± ê²€ì‚¬ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
